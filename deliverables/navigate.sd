project: collect_toys

// def parameters & valid parameter assignments
parameter: int oDestination
available_parameters_code:
__possibleParameters.push_back(std::make_tuple(0));
__possibleParameters.push_back(std::make_tuple(1));
__possibleParameters.push_back(std::make_tuple(2));
__possibleParameters.push_back(std::make_tuple(3));
__possibleParameters.push_back(std::make_tuple(4));


dynamic_model:

 cout << "*Navigate started." << endl
         <<"action cnt="
		 <<(state.action_cnt_dbg)
		 <<"pick_cnt="
         <<(state.pick_cnt)
         <<"nav_cnt="
         <<(state.nav_cnt)
		 <<"total rewards="
		 <<(state.episode_total_rewards_dbg)
		 << "real location = "
         << (state.agentLoc) << " / "
		 << "ball locations="
		 << (state.tBallObjects[0]->location)
         << (state.tBallObjects[1]->location)
         << (state.tBallObjects[2]->location)
         << (state.tBallObjects[3]->location)
		 << endl;
		 
 cout << "Navigate to "
		<<(oDestination)
		 << endl;
		 
//debug
state__.action_cnt_dbg = state.action_cnt_dbg + 1;
state__.nav_cnt++;
vector<float> success_chances{0.5, 0.7, 0.8, 0.85, 0.95};
vector<float> accurate_sensor_chances{0.99, 0.7, 0.8, 0.8, 0.95};
float penalty = 0;
bool leaving_self_location = state.agentLoc != oDestination;
bool is_successful_nav = (leaving_self_location && AOS.Bernoulli(success_chances[oDestination])) ? 1: 0;
// penalty of -1 if navigation failed
float nav_penalty = is_successful_nav ? 0: -1;
bool accurate_sensor = AOS.Bernoulli(accurate_sensor_chances[oDestination]);
// real location after action call - moving only if success
state__.agentLoc = is_successful_nav ? oDestination : state.agentLoc;
bool true_success = is_successful_nav && accurate_sensor;
bool false_failure = !is_successful_nav && !accurate_sensor;
__moduleResponse = true_success || false_failure ? eSuccess : eFailed;


// nav total reward
int r = -1 + nav_penalty;
__reward =  r;
state__.episode_total_rewards_dbg = state.episode_total_rewards_dbg + r;


